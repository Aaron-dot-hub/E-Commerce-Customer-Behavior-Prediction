import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import streamlit as st
import joblib
import os

df=pd.read_csv(r"C:\E-commerce\E-commerce Customer Behavior - Sheet1.csv")
df


# Applied label encoding to the below columns
df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})
df['Discount Applied'] = df['Discount Applied'].map({True: 1, False: 0})
df['Membership Type'] = df['Membership Type'].map( {'Gold': 2, 'Silver': 1, 'Bronze': 0})
df['City'] = df['City'].map({'New York': 0, 'Los Angeles': 1, 'Chicago': 2, 'San Francisco': 3, 'Miami': 4, 'Houston': 5})

#Converted the floats to ints in the below column
df['Total Spend'] = df['Total Spend'].astype(int)
# Check the result
df.head()  # View the first few rows to verify

#Checks if there are any missing values
df.isna().sum()

#Points to the rows where there are missing values
missing_rows = df[df.isnull().any(axis=1)] 
missing_rows

#Fills up the missing values with the most frequent known as mode
df['Satisfaction Level'] = df['Satisfaction Level'].fillna(df['Satisfaction Level'].mode()[0])

#Label encoding is done below
df['Satisfaction Level'] = df['Satisfaction Level'].map({'Satisfied': 1,'Neutral': 0,'Unsatisfied': 2})
df

#Deleting the column 
df.drop(['Customer ID'], axis=1,inplace=True)

df.head()

#Z-Score Normalization to Multiple Columns:
columns_to_normalize = ['Age', 'City', 'Membership Type', 'Total Spend', 'Items Purchased', 'Average Rating', 'Days Since Last Purchase']

# Initialize the scaler
scaler = StandardScaler()

# Apply scaling to the selected columns
df [columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])
df

#print(df['Age'].mean())  # Should be close to 0



# Formatting a single number
#value = 2.0237779706024283e-16
#formatted_value = f"{value:.6f}"
#print(formatted_value)


print(df['Age'].std())   # Should be close to 1

# Elbow Method to determine the best number of clusters (k):
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df)
    inertia.append(kmeans.inertia_)

# Plot the Elbow Method
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

#Kmeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
df['Cluster'] = kmeans.fit_predict(df)
clustered_df = df.copy()  # Save the updated DataFrame to a new variable
clustered_df.to_csv('clustered_data.csv', index=False)
clustered_df

#clustered_df['Cluster'].value_counts()
#(116, 11)=Unsatisfied(2)
#(107,11)=Neutral(0)
#(125,11)=Satisfied(1)

#print(kmeans.cluster_centers_)

# Create a dictionary to map clusters to custom labels
cluster_names = {
    0: 'Medium Value Customers',
    1: 'High Value Customers',
    2: 'Low Value Customers'
}



# Apply PCA to reduce to 2D for visualization
clustered_df = clustered_df.drop('Cluster', axis=1)  # Drop the 'Cluster' column for PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(clustered_df)

# Accessing the centroids
centroids = pca.transform(kmeans.cluster_centers_)  # Transform the centroids to PCA space

# Visualize the clusters in 2D after applying PCA
plt.figure(figsize=(8, 6))

# Plot the data points with cluster labels
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['Cluster'], cmap='viridis', edgecolor='k', s=100, alpha=0.7)

# Plot the centroids
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')



# Create a legend with custom labels
handles, _ = scatter.legend_elements()
labels = [cluster_names[i] for i in sorted(df['Cluster'].unique())]
plt.legend(handles, labels, title="Customer Segments", loc="best")

# Title and labels
plt.title('K-Means Clustering After PCA (Z-score Normalization)', fontsize=16)
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster')
#plt.legend()  # Add a legend to label the centroids

plt.show()


#clustered_df.columns


#x = clustered_df
#y = clustered_df['Cluster']  # The cluster labels generated by KMeans


# Add 'Cluster' column back to PCA-reduced data
#clustered_df = pd.read_csv('clustered_data.csv')
clustered_df['Cluster'] = df['Cluster']  # Add the 'Cluster' column back

# Now you can use the 'Cluster' column as the target (y) in XGBoost
X = clustered_df.drop('Cluster', axis=1)  # Features (PCA components)
y = clustered_df['Cluster']  # Target (Cluster labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert the data into DMatrix (XGBoost optimized format)
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)




# Define the XGBClassifier
xgb_classifier = XGBClassifier(
    objective='multi:softmax',
    num_class=len(y.unique()),
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    n_estimators=100  # Use n_estimators here
)

# Train the model
xgb_classifier.fit(X_train, y_train)

# Save the model
#xgb_classifier.save_model("xgb_model.json")
#import joblib
# Save the model using joblib or XGBoost's built-in method
#joblib.dump(xgb_classifier, "xgb_model.pkl")  # Save as a .pkl file




# Save the model in the current script's directory
save_path = os.path.join(os.path.dirname(__file__), "xgb_model.pkl")
joblib.dump(xgb_classifier, save_path)
print(f"Model saved at {save_path}")


# Make predictions
y_pred = xgb_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))


from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier

# Define your model (XGBClassifier in this case)
model = XGBClassifier(objective='multi:softmax', num_class=3)




# Perform cross-validation (e.g., 5-fold)
cv_scores = cross_val_score(model, X, y, cv=5)  # cv=5 for 5-fold cross-validation

print(f"Cross-validation scores: {cv_scores}")
print(f"Mean cross-validation score: {cv_scores.mean()}")


from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True,fmt="d", cmap="Blues",xticklabels=["Cluster 0", "Cluster 1", "Cluster 2"],yticklabels=["Cluster 0", "Cluster 1", "Cluster 2"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()


from sklearn.metrics import classification_report

print("Classification Report:")
print(classification_report(y_test, y_pred))


from sklearn.model_selection import GridSearchCV

# Define parameter grid for hyperparameter tuning
param_grid = {
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100, 150]
}

# Perform grid search with 5-fold cross-validation
grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print best parameters
print(f"Best parameters: {grid_search.best_params_}")


import matplotlib.pyplot as plt
import xgboost as xgb

# Plot feature importances
#xgb.plot_importance(xgb_classifier)
#plt.show()

# Plot feature importances for the XGBClassifier model
xgb.plot_importance(xgb_classifier, importance_type='weight', max_num_features=10)
plt.title('Feature Importance')
plt.show()
